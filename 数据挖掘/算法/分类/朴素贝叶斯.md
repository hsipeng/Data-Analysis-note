# 朴素贝叶斯



贝叶斯原理

贝叶斯威了解决一个叫“逆向概率”问题写了一片文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。



什么是 “逆向概率”呢？

所谓“逆向概率” 是相对 “正向概率”而言。 正向概率就是袋中摸球

逆向概率，提出了一个问题： 如果我们事先不知道，袋子里的黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里的黑白球的比例。



朴素贝叶斯

它是一种简单但极为强大的预测建模算法

它假设每个输入变量是独立的



朴素贝叶斯由两种类型的概率组成

1. 每个类别的概率 P(Cj)
2. 每个属性的条件概率P(Ai|Cj)
3. 

## Python

对文档进行分词

在准备阶段了，最重要的就是分词。

英文 NTLK 包，中文 jieba

```python
import jieba
# 1. 对文档进行分词
word_list = jieba.cut(text) # 分词
print(word_list)
# 2. 加载停用词表
stop_words = [line.strip() for line in open('datasets/jieba/stop_words.txt' , encoding="utf-8").readlines()]
# 3. 计算权重
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
features = tf.fit_transform(train_contents)


# 4.多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB(alpha=0.01).fill(train_features, train_labels)


# 5. 使用生成的分类器作预测

test_tf = TfidfVectorizer(stop_words=stop_words, max_df = 0.5,
                         vocabulary=train_vocabulary)
test_features = test_tf.fit_transform(test_contents)
# 预测

predicted_labels = clf.predict(test_features)


# 6 计算准确率
from sklearn import metrics
print(metrics.accuracy_score(test_labels, predict_labels))



```





## R



```R
# 朴素贝叶斯
# 通过已知的其他东西，来推断一件事情的发生可能性
# 假定特征之间相互独立

data("iris")

n <- nrow(iris)
ntrain <- round(n*0.6)
set.seed(333)
tindex <- sample(n, ntrain)
train_iris <- iris[tindex,]
test_iris <- iris[-tindex,]
head(train_iris)


library(e1071)

nb1 <- naiveBayes(Species~.,data=train_iris)

nb1

prediction <- predict(nb1, test_iris[,-5])
xtab <- table(prediction, test_iris$Species)
xtab
nb1$apriori # 类的先验概率
nb1$tables$Petal.Length # 高斯分布的平均值和标准差

# 三个类的分布图
# 基于物种的花瓣长度分布
plot(function(x) dnorm(x,1.493548,0.1547805),0,8,lty=1, main="Petal length distribution by species")
curve(dnorm(x,4.306452,0.3855174), add=TRUE, lty=2)
curve(dnorm(x,5.578571,0.5244801), add=TRUE, lty=5)
legend('topright', legend = c("setosa","versicolor","verginica"), lty=c(1,2,5), bty='o')


#例2 income 朴素贝叶斯
# 1. 手动计算
# data
sample <- read.table("sample.csv",header = TRUE,sep = ",")

traindata <- as.data.frame(sample[1:14,])
testdata <- as.data.frame(sample[15,])

traindata
testdata
# Enrolls 先验概率
tprior <- table(traindata$Enrolls)
tprior

tprior <- tprior /sum(tprior)
tprior

# 条件概率
ageCounts <- table(traindata[,c("Enrolls", "Age")])
ageCounts

ageCounts <- ageCounts/rowSums(ageCounts)
ageCounts


incomeCounts <- table(traindata[,c("Enrolls", "Income")])
incomeCounts <- incomeCounts/rowSums(incomeCounts)
incomeCounts

jsCounts <- table(traindata[,c("Enrolls", "JobSatisfaction")])
jsCounts <- jsCounts/rowSums(jsCounts)
jsCounts

desireCounts <- table(traindata[,c("Enrolls", "Desire")])
desireCounts <- desireCounts/rowSums(desireCounts)
desireCounts

# 预测

prob_yes <- ageCounts["Yes",testdata[,c("Age")]]*
  incomeCounts["Yes",testdata[,c("Income")]]*
  jsCounts["Yes",testdata[,c("JobSatisfaction")]]*
  desireCounts["Yes",testdata[,c("Desire")]]*
  tprior["Yes"]

prob_no <- ageCounts["No",testdata[,c("Age")]]*
  incomeCounts["No",testdata[,c("Income")]]*
  jsCounts["No",testdata[,c("JobSatisfaction")]]*
  desireCounts["No",testdata[,c("Desire")]]*
  tprior["No"]
prob_yes
prob_no
max(prob_yes,prob_no)
# 预测结果 Enrolls = Yes

# 2. e1071 内置 naiveBayes
model <- naiveBayes(Enrolls ~ Age + Income + JobSatisfaction + Desire, traindata)

# display model
model

# predict with testdata
results <- predict(model, testdata)
# display results
results

# 3. 拉普拉斯参数
model1 <- naiveBayes(Enrolls ~ ., traindata, laplace = .01)

# display model1
model1

# predict with testdata
results <- predict(model1, testdata)
# display results
results

# 分类诊断, ROC 特征曲线
# data
# age,job,marital,education,default,balance,housing,loan,contact,day,month,duration,campaign,pdays,previous,poutcome,subscribed
# 52,blue-collar,married,secondary,no,557,no,no,unknown,8,may,111,3,-1,0,unknown,no
# 49,blue-collar,married,secondary,no,1264,no,no,cellular,8,aug,140,3,-1,0,unknown,no
# 45,blue-collar,married,unknown,no,248,yes,no,unknown,6,may,88,5,-1,0,unknown,no
#....

# install some packages
install.packages("ROCR") 

library(ROCR)

## Read the data

# training set
banktrain <- read.table("bank-sample.csv",header=TRUE,sep=",")
# drop a few columns
drops <- c("balance", "day", "campaign", "pdays", "previous", "month")
banktrain <- banktrain [,!(names(banktrain) %in% drops)]
# testing set
banktest <- read.table("bank-sample-test.csv",header=TRUE,sep=",")
banktest <- banktest [,!(names(banktest) %in% drops)]
# build the naïve Bayes classifier
nb_model <- naiveBayes(subscribed~.,
                       data=banktrain)
# perform on the testing set
nb_prediction <- predict(nb_model,
                         # remove column "subscribed"
                         banktest[,-ncol(banktest)],
                         type='raw')
score <- nb_prediction[, c("yes")]
actual_class <- banktest$subscribed == 'yes'
pred <- prediction(score, actual_class)
perf <- performance(pred, "tpr", "fpr")

plot(perf, lwd=2, xlab="False Positive Rate (FPR)",
     ylab="True Positive Rate (TPR)")
abline(a=0, b=1, col="gray50", lty=3)

## corresponding AUC score
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
auc

```

