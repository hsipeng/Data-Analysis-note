# 朴素贝叶斯



贝叶斯原理

贝叶斯威了解决一个叫“逆向概率”问题写了一片文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。



什么是 “逆向概率”呢？

所谓“逆向概率” 是相对 “正向概率”而言。 正向概率就是袋中摸球

逆向概率，提出了一个问题： 如果我们事先不知道，袋子里的黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里的黑白球的比例。



朴素贝叶斯

它是一种简单但极为强大的预测建模算法

它假设每个输入变量是独立的



朴素贝叶斯由两种类型的概率组成

1. 每个类别的概率 P(Cj)
2. 每个属性的条件概率P(Ai|Cj)
3. 

## Python

对文档进行分词

在准备阶段了，最重要的就是分词。

英文 NTLK 包，中文 jieba

```python
import jieba
# 1. 对文档进行分词
word_list = jieba.cut(text) # 分词
print(word_list)
# 2. 加载停用词表
stop_words = [line.strip() for line in open('datasets/jieba/stop_words.txt' , encoding="utf-8").readlines()]
# 3. 计算权重
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
features = tf.fit_transform(train_contents)


# 4.多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB(alpha=0.01).fill(train_features, train_labels)


# 5. 使用生成的分类器作预测

test_tf = TfidfVectorizer(stop_words=stop_words, max_df = 0.5,
                         vocabulary=train_vocabulary)
test_features = test_tf.fit_transform(test_contents)
# 预测

predicted_labels = clf.predict(test_features)


# 6 计算准确率
from sklearn import metrics
print(metrics.accuracy_score(test_labels, predict_labels))



```





## R



```R
# 朴素贝叶斯
# 通过已知的其他东西，来推断一件事情的发生可能性
# 假定特征之间相互独立

data("iris")

n <- nrow(iris)
ntrain <- round(n*0.6)
set.seed(333)
tindex <- sample(n, ntrain)
train_iris <- iris[tindex,]
test_iris <- iris[-tindex,]
head(train_iris)


library(e1071)

nb1 <- naiveBayes(Species~.,data=train_iris)

nb1

prediction <- predict(nb1, test_iris[,-5])
xtab <- table(prediction, test_iris$Species)
xtab
nb1$apriori # 类的先验概率
nb1$tables$Petal.Length # 高斯分布的平均值和标准差

# 三个类的分布图
# 基于物种的花瓣长度分布
plot(function(x) dnorm(x,1.493548,0.1547805),0,8,lty=1, main="Petal length distribution by species")
curve(dnorm(x,4.306452,0.3855174), add=TRUE, lty=2)
curve(dnorm(x,5.578571,0.5244801), add=TRUE, lty=5)
legend('topright', legend = c("setosa","versicolor","verginica"), lty=c(1,2,5), bty='o')

```

