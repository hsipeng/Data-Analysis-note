# 经典算法

### 支持向量机 



支持向量机(Support Vector Machine，SVM)是众多监督学习方法中十分出 色的一种，几乎所有讲述经典机器学习方法的教材都会介绍。关于SVM，流传着 一个关于天使与魔鬼的故事。 

传说魔鬼和天使玩了一个游戏，魔鬼在桌上放了两种颜色的球，如图3.1所 示。魔鬼让天使用一根木棍将它们分开。这对天使来说，似乎太容易了。天使不 假思索地一摆，便完成了任务，如图3.2所示。魔鬼又加入了更多的球。随着球的 增多，似乎有的球不能再被原来的木棍正确分开，如图3.3所示。 

知识点 

SVM模型推导，核函数，SMO(Sequential Minimal Optimization)算法 



1. 在空间上线性可分的两类点，分别向**SVM**分类的超平面上做投影，这些点在超平面上的投影仍然是线性可分的吗? 

SVM的分类结果仅依赖于支持向量 

因此SVM的分类结果与仅使用支 持向量的分类结果一致，说明SVM的分类结果仅依赖于支持向量，这也是SVM拥 有极高运行效率的关键之一。于是，我们证明了对于任意线性可分的两组点，它 们在SVM分类的超平面上的投影都是线性不可分的。 

实际上，该问题也可以通过凸优化理论中的超平面分离定理(Separating Hyperplane Theorem，SHT)更加轻巧地解决。该定理描述的是，对于不相交的两 个凸集，存在一个超平面，将两个凸集分离。对于二维的情况，两个凸集间距离 最短两点连线的中垂线就是一个将它们分离的超平面。 

借助这个定理，我们可以先对线性可分的这两组点求各自的凸包。不难发 现，SVM求得的超平面就是两个凸包上距离最短的两点连线的中垂线，也就是 SHT定理二维情况中所阐释的分类超平面。根据凸包的性质容易知道，凸包上的 点要么是样本点，要么处于两个样本点的连线上。因此，两个凸包间距离最短的 两个点可以分为三种情况:两边的点均为样本点，如图3.12(a)所示;两边的点 均在样本点的连线上，如图3.12(b)所示;一边的点为样本点，另一边的点在样 本点的连线上，如图3.12(c)所示。从几何上分析即可知道，无论哪种情况两类 点的投影均是线性不可分的。 



2. 是否存在一组参数使**SVM**训练误差为**0**? 

   存在， 证明略

3. 训练误差为**0**的**SVM**分类器一定存在吗? 

   存在， 证明略

4. 加入松弛变量的**SVM**的训练误差可以为**0**吗? 

   不可以, 当*C*取0时，*w*也取0即可达 到优化目标，但是显然此时我们的训练误差不一定能达到0。 

   证明略



### 逻辑回归 

知识点 

逻辑回归，线性回归，多标签分类，Softmax 



1. 逻辑回归相比于线性回归，有何异同? 

逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者 的最本质的区别。 

逻辑回归中,给定自变量和超参数后，得到因变量的期望，并基于此期望来处理预 测分类问题 

线性回归中实际上求解的是$$y^{'} = \theta^Tx$$，是对我们假设的真实关系$$y=\theta^Tx + \epsilon$$的一个近似，其中$$\epsilon$$代表误差项，我们使用这个近似项来处理回归问 题。 





当然逻辑回归和线性回归也不乏相同之处，首先我们可以认为二者都使用了 极大似然估计来对训练样本进行建模 

二者在求解超参数的过程中，都可以使用梯度下降的方法，这也是 监督学习中一个常见的相似之处。 



2. 当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪 些场景，它们之间又有怎样的关系? 

使用哪一种办法来处理多分类的问题取决于具体问题的定义。首先，如果一 个样本只对应于一个标签，我们可以假设每个样本属于不同标签的概率服从于几 何分布，使用多项逻辑回归(Softmax Regression)来进行分类 

当存在样本可能属于多个标签的情况时，我们可以训练*k*个二分类的逻辑回归 分类器。第*i*个分类器用以区分每个样本是否可以归为第*i*类，训练该分类器时，需 要把标签重新整理为“第*i*类标签”与“非第*i*类标签”两类。通过这样的办法，我们就 解决了每个样本可能拥有多个标签的情况。 



### 决策树 

决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边 组成。结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶 结点表示类别。从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样 本被分到不同的子结点中。再根据子结点的特征进一步划分，直至所有样本都被 归到某一个类别(即叶结点)中。 

决策树作为最基础、最常见的有监督学习模型，常被用于分类问题和回归问 题，在市场营销和生物医药等领域尤其受欢迎，主要因为树形结构与销售、诊断 等场景下的决策过程十分相似。将决策树应用集成学习的思想可以得到随机森 林、梯度提升决策树等模型，这些将在第12章中详细介绍。完全生长的决策树模 型具有简单直观、解释性强的特点，值得读者认真理解，这也是为融会贯通集成 学习相关内容所做的铺垫。 

一般而言，决策树的生成包含了特征选择、树的构造、树的剪枝三个过程， 本节将在第一个问题中对几种常用的决策树进行对比，在第二个问题中探讨决策 树不同剪枝方法之间的区别与联系。 



知识点 

信息论，树形数据结构，优化理论 



1. 决策树有哪些常用的启发函数? 

常用的决策树算法有ID3、C4.5、CART，它们构建树所使用的启发式函数各 是什么?除了构建准则之外，它们之间的区别与联系是什么? 

**■ ID3——** 最大信息增益 

**■ C4.5——**最大信息增益比 

**■ CART——**最大基尼指数(**Gini**) 



通过对比三种决策树的构造准则，以及在同一例子上的不同表现，我们不难 总结三者之间的差异。 

首先，ID3是采用信息增益作为评价标准，除了“会写代码”这一逆天特征外， 会倾向于取值较多的特征。 

其次，从样本类型的角度，ID3只能处理离散型变量，而C4.5和CART都可以 处理连续型变量。 

从应用角度，ID3和C4.5只能用于分类任务，而CART(Classification and Regression Tree，分类回归树)从名字就可以看出其不仅可以用于分类，也可以应 用于回归任务(回归树使用最小平方误差准则)。 

此外，从实现细节、优化过程等角度，这三种决策树还有一些不同。比如， ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处 理;ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复 用，而CART每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特 征可以被重复使用;ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART 直接利用全部数据发现所有可能的树结构进行对比。 



2. 如何对决策树进行剪枝? 

决策树的剪枝通常有两种方法，预剪枝(Pre-Pruning)和后剪枝(Post- Pruning)。那么这两种方法是如何进行的呢?它们又各有什么优缺点? 

预剪枝，即在生成决策树的过程中提前停止树的增长。而后剪枝，是在已生 成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。 

**■** 预剪枝 

预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带 来模型泛化能力的提升，如果不能，则不再继续生长子树。此时可能存在不同类 别的样本同时存于结点中，按照多数投票的原则判断该结点所属类别。预剪枝对 于何时停止决策树的生长有以下几种方法。 

(1)当树到达一定深度的时候，停止树的生长。 

(2)当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。 

(3)计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继 续扩展。 

预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模问题。但 如何准确地估计何时停止树的生长(即上述方法中的深度或阈值)，针对不同问 题会有很大差别，需要一定经验判断。且预剪枝存在一定局限性，有欠拟合的风 险，虽然当前的划分会导致测试集准确率降低，但在之后的划分中，准确率可能 会有显著上升。 

**■** 后剪枝 后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上 

计算是否剪枝。剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样 按照多数投票的原则进行判断。同样地，后剪枝也可以通过在测试集上的准确率 进行判断，如果剪枝过后准确率有所提升，则进行剪枝。相比于预剪枝，后剪枝 方法通常可以得到泛化能力更强的决策树，但时间开销会更大。 

常见的后剪枝方法包括错误率降低剪枝(Reduced Error Pruning，REP)、悲 观剪枝(Pessimistic Error Pruning，PEP)、代价复杂度剪枝(Cost Complexity Pruning，CCP)、最小误差剪枝(Minimum Error Pruning，MEP)、CVP(Critical Value Pruning)、OPP(Optimal Pruning)等方法，这些剪枝方法各有利弊，关注 不同的优化角度，本文选取著名的CART剪枝方法CCP进行介绍。 

代价复杂剪枝主要包含以下两个步骤。 (1)从完整决策树*T**0*开始，生成一个子树序列{*T**0*,*T**1*,*T**2*,...,*T**n*}，其中*T**i+1*由*T**i*生 

成，*T**n*为树的根结点。 (2)在子树序列中，根据真实误差选择最佳的决策树。 

步骤(1)从*T**0*开始，裁剪*T**i*中关于训练数据集合误差增加最小的分支以得 到*T**i+1*。具体地，当一棵树*T*在结点*t*处剪枝时，它的误差增加可以用*R*(*t*)−*R*(*T**t*)表 示，其中*R*(*t*)表示进行剪枝之后的该结点误差，*R*(*T**t*)表示未进行剪枝时子树*T**t*的误 差。考虑到树的复杂性因素，我们用|*L*(*T**t*)|表示子树*T**t*的叶子结点个数，则树在结 点*t*处剪枝后的误差增加率为 





